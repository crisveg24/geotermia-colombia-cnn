# ğŸ”§ Mejoras Sugeridas para el Modelo CNN

## Estado Actual del CÃ³digo

El cÃ³digo del modelo `cnn_geotermia.py` estÃ¡ **bien estructurado** y sigue buenas prÃ¡cticas:
- âœ… Arquitectura modular con bloques residuales
- âœ… Batch Normalization y Dropout
- âœ… RegularizaciÃ³n L2
- âœ… Global Average Pooling
- âœ… DocumentaciÃ³n completa
- âœ… Soporte para Transfer Learning

## Mejoras Recomendadas

### 1. ğŸ¯ Mejoras en la Arquitectura

```python
# ANTES: Dropout fijo
x = layers.Dropout(self.dropout_rate * 0.5)(x)

# DESPUÃ‰S: Dropout adaptativo por capa (Spatial Dropout para Conv)
x = layers.SpatialDropout2D(self.dropout_rate * 0.3)(x)  # MÃ¡s efectivo para CNNs
```

### 2. ğŸ“Š Data Augmentation Mejorado

Agregar en `train_model.py`:
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.3),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2),
    # Nuevo: augmentaciÃ³n especÃ­fica para imÃ¡genes tÃ©rmicas
    layers.GaussianNoise(0.05),
])
```

### 3. ğŸ§  Attention Mechanism (Mejora significativa)

Agregar Spatial Attention despuÃ©s de los bloques residuales:
```python
def _attention_block(self, x, name=""):
    """Bloque de atenciÃ³n espacial."""
    # Channel attention
    avg_pool = layers.GlobalAveragePooling2D()(x)
    max_pool = layers.GlobalMaxPooling2D()(x)
    
    dense = layers.Dense(x.shape[-1] // 16, activation='relu')
    avg_out = dense(avg_pool)
    max_out = dense(max_pool)
    
    channel_attention = layers.Dense(x.shape[-1], activation='sigmoid')(avg_out + max_out)
    x = x * tf.reshape(channel_attention, [-1, 1, 1, x.shape[-1]])
    
    return x
```

### 4. ğŸ“ˆ Learning Rate Scheduling Mejorado

```python
# Usar Cosine Annealing con Warm Restarts
lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate=0.001,
    first_decay_steps=1000,
    t_mul=2.0,
    m_mul=0.9
)
```

### 5. ğŸ”„ Mixup y CutMix (Data Augmentation Avanzado)

```python
def mixup(images, labels, alpha=0.2):
    """Aplica Mixup augmentation."""
    batch_size = tf.shape(images)[0]
    lam = np.random.beta(alpha, alpha)
    indices = tf.random.shuffle(tf.range(batch_size))
    
    mixed_images = lam * images + (1 - lam) * tf.gather(images, indices)
    mixed_labels = lam * labels + (1 - lam) * tf.gather(labels, indices)
    
    return mixed_images, mixed_labels
```

### 6. ğŸ“Š MÃ©tricas Adicionales

```python
# Agregar mÃ©tricas especÃ­ficas para geotermia
metrics = [
    'accuracy',
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
    keras.metrics.AUC(curve='PR', name='auc_pr'),  # Mejor para desbalanceo
    tfa.metrics.F1Score(num_classes=2, average='macro', name='f1'),  # F1 directo
]
```

### 7. ğŸ›ï¸ Manejo de Desbalanceo de Clases

```python
# Calcular pesos de clase automÃ¡ticamente
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))

# Usar en fit()
model.fit(X_train, y_train, class_weight=class_weight_dict, ...)
```

### 8. ğŸ” Explainability (Interpretabilidad)

Agregar Grad-CAM para visualizar quÃ© Ã¡reas del imagen activan el modelo:
```python
import tensorflow as tf

def make_gradcam_heatmap(model, img_array, last_conv_layer_name):
    """Genera heatmap de activaciÃ³n."""
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        loss = predictions[:, 0]
    
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    
    return heatmap.numpy()
```

## Prioridad de ImplementaciÃ³n

| Mejora | Impacto | Dificultad | Prioridad |
|--------|---------|------------|-----------|
| Data Augmentation mejorado | Alto | Baja | ğŸ”´ Alta |
| Manejo desbalanceo | Alto | Baja | ğŸ”´ Alta |
| Learning Rate Scheduling | Medio | Baja | ğŸŸ¡ Media |
| Attention Mechanism | Alto | Media | ğŸŸ¡ Media |
| Mixup/CutMix | Medio | Media | ğŸŸ¢ Baja |
| Grad-CAM | Bajo | Media | ğŸŸ¢ Baja |

## PrÃ³ximos Pasos

1. **Inmediato:** Aplicar class weights y mejor data augmentation
2. **Corto plazo:** Implementar learning rate scheduling mejorado
3. **Medio plazo:** Agregar attention mechanism
4. **Largo plazo:** Implementar Grad-CAM para interpretabilidad

---

**Nota:** Estas mejoras son opcionales. El modelo actual ya tiene una buena base y puede lograr buenos resultados con suficientes datos de entrenamiento.
